import boto3
import json
import urllib.parse
import csv
import pandas as pd
import numpy as np
import io
from datetime import datetime

s3 = boto3.client('s3')
RAW_BUCKET = 'raw-bucket-891377383993' 
TRUSTED_BUCKET = 'trusted-bucket-891377383993'
CLIENT_BUCKET = 'client-bucket-891377383993'

def lambda_handler(event, context):
    for record in event['Records']:
        source_bucket = record['s3']['bucket']['name']
        key = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')
        print(f"Processando arquivo: s3://{source_bucket}/{key}")

        try:
            # Processa a base ANEEL espec√≠fica
            aneel_csv_key = aneel_raw_to_trusted(s3, RAW_BUCKET, TRUSTED_BUCKET)
            if aneel_csv_key:
                print(f"‚Üí Chamando fun√ß√£o de cliente para ANEEL: {aneel_csv_key}")
            raw_to_trusted(source_bucket, key)
            trusted_to_client(key)
        except Exception as e:
            print(f"function=lambda_handler_error file={key} message={e}")

    



def raw_to_trusted(source_bucket, key):
    try:
        # L√™ o JSON do bucket de origem
        response = s3.get_object(Bucket=source_bucket, Key=key)
        raw_bytes = response['Body'].read()

        # Tenta decodificar o conte√∫do
        try:
            content = raw_bytes.decode('utf-8')
        except UnicodeDecodeError:
            content = raw_bytes.decode('latin1')

        # Remove BOM se houver
        content = content.lstrip('\ufeff')

        # Converte o conte√∫do JSON
        data = json.loads(content)
        if isinstance(data, dict):
            data = [data]

        print(f"Total de registros lidos: {len(data)}")

        valid_rows = []
        for row in data:
            if not row:
                continue

            data_captura = row.get("data_captura")
            if not data_captura:
                continue

            try:
                data_dt = datetime.strptime(data_captura, "%Y-%m-%d %H:%M:%S")
                dia_captura = data_dt.strftime("%Y-%m-%d")
                hora_captura = data_dt.strftime("%H:%M:%S")
            except ValueError:
                print(f"Formato de data inv√°lido em {data_captura}, arquivo: {key}")
                continue

            valid_rows.append({
                "corrente": row.get("sensor_1"),
                "tensao": row.get("sensor_2"),
                "temperatura": row.get("sensor_3"),
                "vibracao": row.get("sensor_4"),
                "pressao": row.get("sensor_5"),
                "frequencia": row.get("sensor_6"),
                "dia_captura": dia_captura,
                "hora_captura": hora_captura
            })

        if not valid_rows:
            print(f"Nenhum dado v√°lido encontrado no arquivo {key}.")
            return

        # Cria o CSV em mem√≥ria
        output = io.StringIO()
        writer = csv.DictWriter(output, fieldnames=[
            "corrente", "tensao", "temperatura", "vibracao", "pressao", "frequencia",
            "dia_captura", "hora_captura"
        ])
        writer.writeheader()
        writer.writerows(valid_rows)
        csv_data = output.getvalue().encode('utf-8')
        output.close()

        # Define o nome do arquivo no bucket trusted
        csv_key = key.replace('.json', '.csv')

        # Envia para o bucket trusted
        s3.put_object(
            Bucket=TRUSTED_BUCKET,
            Key=csv_key,
            Body=csv_data,
            ContentType='text/csv'
        )

        print(f"CSV salvo no bucket '{TRUSTED_BUCKET}' como '{csv_key}'")

    except Exception as e:
        print(f"function=raw_to_trusted_error file={key} message={e}")

def trusted_to_client(key):
    try:
        csv_key = key.replace('.json', '.csv')

        # 1Ô∏è‚É£ L√™ o CSV do trusted e cria o DataFrame
        obj = s3.get_object(Bucket=TRUSTED_BUCKET, Key=csv_key)
        df = pd.read_csv(io.BytesIO(obj['Body'].read()))

        # 2Ô∏è‚É£ Chama todas as fun√ß√µes de tratamento, passando o df
        for func in [corrente, tensao, temperatura, vibracao, pressao, frequencia]:
            try:
                df = func(df)  # <- altera√ß√£o aqui
            except Exception as e:
                print(f"Erro na fun√ß√£o {func.__name__}: {e}")

        
        # 3Ô∏è‚É£ Enriquecer com dados da ANEEL
        try:
            df = gerar_flat_table_aneel(df, s3)
        except Exception as e:
            print(f"Erro ao integrar ANEEL na flat table: {e}")

        # --- Normaliza√ß√£o para Athena (evitar HIVE_BAD_DATA) ---

        double_cols = [
            "corrente","tensao","temperatura","vibracao","pressao","frequencia",
            "carga_media_trabalho_amps","mtbf_minutos","mttr_minutos",
            "perc_tempo_desligada","perc_tempo_em_carga","perc_tempo_ociosa",
            "confiabilidade_perc_oee","ANEEL_Nivel_Tensao"
        ]

        int_cols = [
            "total_eventos_sobrecarga",
            "ANEEL_Qtd_UC_Afetadas",
            "ANEEL_Qtd_Consumidores_Afetados"
        ]

        bool_cols = ["alerta_sobrecarga","falha_energia"]

        # Converte para DOUBLE (valores inv√°lidos ficam como NaN ‚Üí Athena interpreta como NULL)
        for col in double_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # Converte para INT seguro (preserva NULL)
        for col in int_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

        # Converte booleanos de forma segura
        for col in bool_cols:
            if col in df.columns:
                df[col] = df[col].replace({None: False, "": False}).astype(bool)

        # Garantir que strings vazias virem NULL (n√£o "nan")
        df = df.replace({np.nan: None})


        # 3Ô∏è‚É£ Salva o DataFrame final tratado no client bucket
        out = io.StringIO()
        df.to_csv(out, index=False)
        s3.put_object(
            Bucket=CLIENT_BUCKET,
            Key=csv_key,
            Body=out.getvalue().encode('utf-8'),
            ContentType='text/csv'
        )

        print(f"CSV copiado com sucesso para o bucket '{CLIENT_BUCKET}' como '{csv_key}'")
    except Exception as e:
        print(f"function=trusted_to_client_error file={key} message={e}")

def aneel_raw_to_trusted(s3, raw_bucket, trusted_bucket):

    ANEEL_KEY = "falhas_energia_sbc.csv"

    try:
        # 1Ô∏è‚É£ Ler o arquivo do bucket RAW
        response = s3.get_object(Bucket=raw_bucket, Key=ANEEL_KEY)
        raw_bytes = response['Body'].read()

        # Detectar encoding automaticamente
        try:
            csv_content = raw_bytes.decode('utf-8')
        except UnicodeDecodeError:
            csv_content = raw_bytes.decode('latin1')

        df = pd.read_csv(io.StringIO(csv_content))
        print(f"‚úÖ CSV lido com {len(df)} linhas e {len(df.columns)} colunas")

        # 2Ô∏è‚É£ Remover colunas irrelevantes se existirem
        colunas_remover = [
            '_id', 'DatGeracaoConjuntoDados', 'NumOrdemInterrupcao',
            'IdeMotivoInterrupcao', 'numCPF', 'rank'
        ]
        df.drop(columns=[c for c in colunas_remover if c in df.columns], inplace=True, errors='ignore')

        # 3Ô∏è‚É£ Limpar registros totalmente nulos
        df.dropna(how='all', inplace=True)
        df.replace({np.nan: None}, inplace=True)

        # 4Ô∏è‚É£ Filtrar por Alimentador "SBC 0113"
        if "DscAlimentadorSubestacao" in df.columns:
            df = df[df["DscAlimentadorSubestacao"].str.contains("0113", case=False, na=False)]
        else:
            print("‚ö†Ô∏è Coluna 'DscAlimentadorSubestacao' n√£o encontrada ‚Äî nenhum filtro aplicado.")

        print(f"üìä Linhas ap√≥s filtro SBC 0113: {len(df)}")

        if df.empty:
            print("‚ö†Ô∏è Nenhum registro encontrado com Alimentador SBC 0113.")
        
        # 5Ô∏è‚É£ Separar e renomear colunas de data/hora
        def separar_data_hora(df, coluna_original, prefixo):
            if coluna_original not in df.columns:
                return df
            df[coluna_original] = pd.to_datetime(df[coluna_original], errors='coerce')
            df[f"ANEEL_{prefixo}_Data"] = df[coluna_original].dt.strftime("%Y-%m-%d")
            df[f"ANEEL_{prefixo}_Hora"] = df[coluna_original].dt.strftime("%H:%M")  # ‚¨ÖÔ∏è sem segundos
            df.drop(columns=[coluna_original], inplace=True)
            return df

        df = separar_data_hora(df, "DatInicioInterrupcao", "Inicio_Interrupcao")
        df = separar_data_hora(df, "DatFimInterrupcao", "Fim_Interrupcao")

        # 6Ô∏è‚É£ Renomear colunas principais (com prefixo ANEEL_)
        rename_map = {
            'IdeConjuntoUnidadeConsumidora': 'ANEEL_ID_Conjunto_UC',
            'DscConjuntoUnidadeConsumidora': 'ANEEL_Nome_Conjunto_UC',
            'DscAlimentadorSubestacao': 'ANEEL_Alimentador',
            'DscSubestacaoDistribuicao': 'ANEEL_Subestacao',
            'DscTipoInterrupcao': 'ANEEL_Tipo_Interrupcao',
            'DscFatoGeradorInterrupcao': 'ANEEL_Fato_Gerador',
            'NumNivelTensao': 'ANEEL_Nivel_Tensao',
            'NumUnidadeConsumidora': 'ANEEL_Qtd_UC_Afetadas',
            'NumConsumidorConjunto': 'ANEEL_Qtd_Consumidores_Afetados',
            'NumAno': 'ANEEL_Ano',
            'NomAgenteRegulado': 'ANEEL_Agente_Regulado',
            'SigAgente': 'ANEEL_Sigla_Agente'
        }

        df.rename(columns=rename_map, inplace=True)

        # 7Ô∏è‚É£ Limpeza final: remover duplicatas e resetar √≠ndices
        df.drop_duplicates(inplace=True)
        df.reset_index(drop=True, inplace=True)

        # 8Ô∏è‚É£ Salvar no bucket TRUSTED
        aneel_trusted_key = ANEEL_KEY.replace('.csv', '_trusted_vw.csv')
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)

        s3.put_object(
            Bucket=trusted_bucket,
            Key=aneel_trusted_key,
            Body=csv_buffer.getvalue().encode('utf-8'),
            ContentType='text/csv'
        )

        print(f"‚úÖ Arquivo tratado salvo em s3://{trusted_bucket}/{aneel_trusted_key}")
        return aneel_trusted_key

    except Exception as e:
        print(f"‚ùå function=aneel_raw_to_trusted_error message={e}")
        return None

def gerar_flat_table_aneel(df, s3):
    TRUSTED_BUCKET = "trusted-bucket-891377383993"
    ANEEL_KEY = "falhas_energia_sbc_trusted_vw.csv"

    # 1) Carrega dados ANEEL tratados
    obj = s3.get_object(Bucket=TRUSTED_BUCKET, Key=ANEEL_KEY)
    aneel = pd.read_csv(io.BytesIO(obj["Body"].read()))

    # 2) Cria timestamps completos para intervalo da falha
    aneel["aneel_inicio_interrupcao"] = pd.to_datetime(
        aneel["ANEEL_Inicio_Interrupcao_Data"] + " " + aneel["ANEEL_Inicio_Interrupcao_Hora"],
        format="%Y-%m-%d %H:%M",
        errors="coerce"
    )
    aneel["aneel_fim_interrupcao"] = pd.to_datetime(
        aneel["ANEEL_Fim_Interrupcao_Data"] + " " + aneel["ANEEL_Fim_Interrupcao_Hora"],
        format="%Y-%m-%d %H:%M",
        errors="coerce"
    )

    # 3) Normaliza o timestamp do sensor
    df["data_captura"] = pd.to_datetime(df["dia_captura"] + " " + df["hora_captura"], errors="coerce")

    # 4) Inicializa colunas ANEEL na flat table (garantido)
    aneel_cols = [
        "ANEEL_Subestacao", "ANEEL_Alimentador", "ANEEL_Tipo_Interrupcao",
        "ANEEL_Fato_Gerador", "ANEEL_Nivel_Tensao", "ANEEL_Qtd_UC_Afetadas",
        "ANEEL_Qtd_Consumidores_Afetados", "ANEEL_Agente_Regulado"
    ]
    for c in aneel_cols:
        if c not in df.columns:
            df[c] = None

    # 5) Adiciona flags e timestamps da falha
    df["falha_energia"] = False
    df["aneel_inicio_interrupcao"] = None
    df["aneel_fim_interrupcao"] = None

    # 6) Preenche para cada intervalo de falha ANEEL
    for _, r in aneel.iterrows():
        mask = (df["data_captura"] >= r["aneel_inicio_interrupcao"]) & (df["data_captura"] <= r["aneel_fim_interrupcao"])
        df.loc[mask, aneel_cols] = r[aneel_cols].values
        df.loc[mask, "falha_energia"] = True
        df.loc[mask, "aneel_inicio_interrupcao"] = r["aneel_inicio_interrupcao"]
        df.loc[mask, "aneel_fim_interrupcao"] = r["aneel_fim_interrupcao"]

    print(f"‚úì ANEEL integrada ao dataset na granularidade do sensor ({len(aneel)} falhas aplicadas)")
    return df




def corrente(df):
    print("‚Üí Calculando m√©tricas de corrente...")

    LIMITE_DESLIGADA = 0.5
    LIMITE_TRABALHO = 10.0
    LIMITE_SOBRECARGA = 50.0

    df['data_captura'] = pd.to_datetime(df['dia_captura'] + ' ' + df['hora_captura'])
    df = df.sort_values(by='data_captura').reset_index(drop=True)

    def definir_estado(c):
        if c < LIMITE_DESLIGADA:
            return "Desligada"
        elif c >= LIMITE_TRABALHO:
            return "Em Carga"
        else:
            return "Ociosa"

    df['estado_operacional'] = df['corrente'].apply(definir_estado)
    df['alerta_sobrecarga'] = df['corrente'] > LIMITE_SOBRECARGA
    df['duracao_segundos'] = df['data_captura'].diff().dt.total_seconds()
    df['duracao_segundos'].fillna(df['duracao_segundos'].mean(), inplace=True)

    tempo_total = df['duracao_segundos'].sum()
    tempo_por_estado = df.groupby('estado_operacional')['duracao_segundos'].sum()
    perc_em_carga = (tempo_por_estado.get('Em Carga', 0) / tempo_total) * 100
    perc_ociosa = (tempo_por_estado.get('Ociosa', 0) / tempo_total) * 100
    perc_desligada = (tempo_por_estado.get('Desligada', 0) / tempo_total) * 100

    df['estado_mtbf'] = np.where(df['estado_operacional'] == 'Em Carga', 'UP', 'DOWN')
    df['mudou_estado'] = df['estado_mtbf'].shift() != df['estado_mtbf']
    df.loc[0, 'mudou_estado'] = True
    df['grupo'] = df['mudou_estado'].cumsum()

    duracao_eventos = df.groupby('grupo').agg(
        estado=('estado_mtbf', 'first'),
        duracao_total=('duracao_segundos', 'sum')
    )

    uptime = duracao_eventos[duracao_eventos['estado'] == 'UP']['duracao_total']
    downtime = duracao_eventos[duracao_eventos['estado'] == 'DOWN']['duracao_total']

    mtbf_minutos = (uptime.mean() / 60) if not uptime.empty else 0
    mttr_minutos = (downtime.mean() / 60) if not downtime.empty else 0

    total_uptime = uptime.sum()
    total_downtime = downtime.sum()

    confiabilidade_perc = (
        (total_uptime / (total_uptime + total_downtime)) * 100
        if (total_uptime + total_downtime) > 0
        else 100
    )

    df_carga = df[df['estado_operacional'] == 'Em Carga']
    carga_media_trabalho_amps = df_carga['corrente'].mean() if not df_carga.empty else 0
    total_eventos_sobrecarga = df['alerta_sobrecarga'].sum()

    df['carga_media_trabalho_amps'] = carga_media_trabalho_amps
    df['mtbf_minutos'] = mtbf_minutos
    df['mttr_minutos'] = mttr_minutos
    df['perc_tempo_desligada'] = perc_desligada
    df['perc_tempo_em_carga'] = perc_em_carga
    df['perc_tempo_ociosa'] = perc_ociosa
    df['confiabilidade_perc_oee'] = confiabilidade_perc
    df['total_eventos_sobrecarga'] = total_eventos_sobrecarga

    df.drop(columns=['duracao_segundos', 'estado_mtbf', 'mudou_estado', 'grupo'], inplace=True)
    
    print("‚úì Corrente processada com sucesso.")
    return df

def tensao(df):
    return df

def temperatura(df):
    return df   

def vibracao(df):
    return df   

def pressao(df):
    print("‚Üí Iniciando jun√ß√£o com reclama√ß√µes...")
    
    try:
        # 1Ô∏è‚É£ Nome do bucket e arquivo
        bucket = CLIENT_BUCKET
        key = "client_reclamacoes_bruto.csv"

        # 2Ô∏è‚É£ L√™ o CSV de reclama√ß√µes do bucket client
        response = s3.get_object(Bucket=bucket, Key=key)
        raw_bytes = response['Body'].read()

        # Detecta encoding automaticamente
        try:
            csv_content = raw_bytes.decode('utf-8')
        except UnicodeDecodeError:
            csv_content = raw_bytes.decode('latin1')

        df_reclamacoes = pd.read_csv(io.StringIO(csv_content))

        # 3Ô∏è‚É£ Tratar a coluna 'created' ‚Äî deixar s√≥ a data
        df_reclamacoes['created'] = pd.to_datetime(df_reclamacoes['created'], errors='coerce').dt.date.astype(str)

        # 4Ô∏è‚É£ Renomear todas as colunas (coloque os novos nomes aqui)
        df_reclamacoes.rename(columns={
            'title': 'reclamacao_titulo',
            'description': 'reclamacao_descricao',
            'userstate': 'reclamacao_estado',
            'usercity': 'reclamacao_cidade',
            'status': 'reclamacao_status',
            'created': 'dia_captura',   # j√° padroniza a data para o join
            'url': 'reclamacao_url'
        }, inplace=True)

        print(f"‚úì Colunas renomeadas: {list(df_reclamacoes.columns)}")

        # 5Ô∏è‚É£ Fazer FULL OUTER JOIN (mant√©m todos os dias)
        df_flat = pd.merge(df, df_reclamacoes, on='dia_captura', how='outer')

        print(f"‚úì Jun√ß√£o conclu√≠da. Total de registros combinados: {len(df_flat)}")

        # 6Ô∏è‚É£ Salvar no bucket client
        out = io.StringIO()
        df_flat.to_csv(out, index=False)
        s3.put_object(
            Bucket=bucket,
            Key='flat_table.csv',
            Body=out.getvalue().encode('utf-8'),
            ContentType='text/csv'
        )

        print("‚úì Flat table salva no bucket client com sucesso.")
        return df_flat

    except Exception as e:
        print(f"function=pressao_error message={e}")
        return df



def frequencia(df):
    return df